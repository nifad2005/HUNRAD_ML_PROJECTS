{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fdc56d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\nifad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nifad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\nifad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\nifad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\nifad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- ধাপ ১: ডামি ডেটাসেট তৈরি করো (বাস্তবে তুমি একটি বড় ডেটাসেট লোড করবে) ---\n",
    "# এই ডেটাসেটটি ছোট আকারের, শুধুমাত্র উদাহরণের জন্য।\n",
    "# বাস্তব ডেটাসেট: যেমন IMDB মুভি রিভিউ ডেটাসেট (অনেক বড়)\n",
    "data = {\n",
    "    'review': [\n",
    "        \"This movie was absolutely fantastic! A must-watch.\",\n",
    "        \"Terrible acting and a boring plot. Don't waste your time.\",\n",
    "        \"A decent film, but nothing extraordinary.\",\n",
    "        \"I loved every minute of it, truly a masterpiece.\",\n",
    "        \"Worst movie ever, completely unwatchable.\",\n",
    "        \"Surprisingly good, exceeded my expectations.\",\n",
    "        \"Mediocre at best, very predictable.\",\n",
    "        \"Highly recommended, great story and visuals.\",\n",
    "        \"So disappointing, felt like a chore to finish.\",\n",
    "        \"Enjoyed it thoroughly, great performances.\",\n",
    "        \"It was okay, not bad, not great.\",\n",
    "        \"Absolutely brilliant, a cinematic triumph.\",\n",
    "        \"What a mess, confusing and poorly executed.\",\n",
    "        \"Very engaging and thought-provoking.\",\n",
    "        \"Couldn't stand it, walked out halfway.\"\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'positive', 'negative', 'neutral', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'neutral', 'positive', 'negative', 'positive', 'negative'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 'neutral' রিভিউগুলোকে বাদ দিচ্ছি বা পজিটিভ/নেগেটিভে অ্যাসাইন করছি\n",
    "# সেন্টিমেন্ট অ্যানালাইসিসের জন্য সাধারণত বাইনারি ক্লাসিফিকেশন (পজিটিভ/নেগেটিভ) করা হয়।\n",
    "# এখানে 'neutral' রিভিউগুলোকে বাদ দিচ্ছি বোঝার সুবিধার জন্য।\n",
    "df = df[df['sentiment'] != 'neutral']\n",
    "\n",
    "# সেন্টিমেন্ট লেবেলগুলোকে সংখ্যায় রূপান্তর করো: 'positive' -> 1, 'negative' -> 0\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "print(\"প্রসেস করা ডেটাসেটের প্রথম ৫টি সারি:\")\n",
    "print(df.head())\n",
    "print(f\"\\nমোট রিভিউ সংখ্যা: {len(df)}\")\n",
    "\n",
    "# ফিচার (X) এবং টার্গেট (y) আলাদা করো\n",
    "X_text = df['review'].values\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# --- ধাপ ২: টেক্সট ডেটা প্রিপারেশন (Tokenization and Padding) ---\n",
    "\n",
    "# টোকেনাইজার তৈরি করো: এটি শব্দগুলোকে সংখ্যায় রূপান্তর করবে\n",
    "# num_words: কতগুলো সবচেয়ে বেশি ব্যবহৃত শব্দ বিবেচনা করা হবে\n",
    "# oov_token: অজানা শব্দগুলোর জন্য একটি টোকেন (Out-Of-Vocabulary)\n",
    "vocab_size = 1000 # তুমি ডেটাসেটের আকারের উপর নির্ভর করে এটি বাড়াতে পারো\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(X_text) # টেক্সট ডেটার উপর টোকেনাইজারকে ফিট করো\n",
    "\n",
    "# টেক্সটগুলোকে সংখ্যার সিকোয়েন্সে রূপান্তর করো\n",
    "X_sequences = tokenizer.texts_to_sequences(X_text)\n",
    "\n",
    "# সিকোয়েন্সগুলোর দৈর্ঘ্য সমান করো (প্যাডিং)\n",
    "# maxlen: প্রতিটি সিকোয়েন্সের সর্বোচ্চ দৈর্ঘ্য\n",
    "# padding='post': যদি সিকোয়েন্স ছোট হয়, তাহলে শেষে শূন্য যোগ করা হবে\n",
    "# truncating='post': যদি সিকোয়েন্স বড় হয়, তাহলে শেষে কেটে ফেলা হবে\n",
    "maxlen = 50 # প্রতিটি রিভিউয়ের সর্বোচ্চ শব্দ সংখ্যা\n",
    "X_padded = pad_sequences(X_sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "print(f\"\\nপ্রথম রিভিউয়ের মূল টেক্সট: {X_text[0]}\")\n",
    "print(f\"প্রথম রিভিউয়ের টোকেনাইজড সিকোয়েন্স: {X_sequences[0]}\")\n",
    "print(f\"প্রথম রিভিউয়ের প্যাডেড সিকোয়েন্স (maxlen={maxlen}): {X_padded[0]}\")\n",
    "\n",
    "# --- ধাপ ৩: ডেটা সেটকে ট্রেনিং এবং টেস্টিং সেটে ভাগ করো ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nট্রেনিং ডেটার আকার: {X_train.shape}\")\n",
    "print(f\"টেস্টিং ডেটার আকার: {X_test.shape}\")\n",
    "\n",
    "# --- ধাপ ৪: ডিপ লার্নিং মডেল তৈরি করো (LSTM ব্যবহার করে) ---\n",
    "embedding_dim = 100 # প্রতিটি শব্দের জন্য এমবেডিং ভেক্টরের মাত্রা\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding Layer: শব্দ ইনডেক্সগুলোকে ঘন ভেক্টরে রূপান্তর করে\n",
    "# input_dim: শব্দভাণ্ডারের আকার (vocab_size)\n",
    "# output_dim: এমবেডিং ভেক্টরের মাত্রা\n",
    "# input_length: প্রতিটি ইনপুট সিকোয়েন্সের দৈর্ঘ্য (maxlen)\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "\n",
    "# LSTM Layer: সিকোয়েন্স ডেটা প্রসেস করার জন্য\n",
    "# 128: LSTM ইউনিটের সংখ্যা (তুমি এটি পরিবর্তন করতে পারো)\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Dropout Layer: ওভারফিটিং কমানোর জন্য (কিছু নিউরনকে র্যান্ডমলি বন্ধ করে দেয়)\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Dense Output Layer: বাইনারি ক্লাসিফিকেশনের জন্য 1টি নিউরন\n",
    "# activation='sigmoid': আউটপুটকে 0 থেকে 1 এর মধ্যে সম্ভাবনায় রূপান্তর করে\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# মডেলের সারসংক্ষেপ দেখাও\n",
    "model.summary()\n",
    "\n",
    "# --- ধাপ ৫: মডেল কম্পাইল করো ---\n",
    "# optimizer: মডেলকে কিভাবে শিখতে হবে তা নিয়ন্ত্রণ করে (Adam একটি জনপ্রিয় অপ্টিমাইজার)\n",
    "# loss: মডেলের প্রেডিকশন এবং আসল লেবেলের মধ্যে পার্থক্য পরিমাপ করে (বাইনারি ক্লাসিফিকেশনের জন্য binary_crossentropy)\n",
    "# metrics: ট্রেনিং এবং মূল্যায়নের সময় মডেলের পারফরম্যান্স পরিমাপ করার জন্য (accuracy)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- ধাপ ৬: মডেল ট্রেনিং করো ---\n",
    "# EarlyStopping: যদি মডেলের পারফরম্যান্স (validation loss) নির্দিষ্ট সংখ্যক এপক ধরে উন্নত না হয়, তাহলে ট্রেনিং বন্ধ করে দেবে।\n",
    "# patience: কতগুলো এপক ধরে উন্নতি না হলে ট্রেনিং বন্ধ হবে\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "print(\"\\nমডেল ট্রেনিং শুরু হচ্ছে...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50, # তুমি আরও বেশি এপক দিতে পারো, EarlyStopping এটি নিয়ন্ত্রণ করবে\n",
    "    batch_size=32, # একবারে কতগুলো স্যাম্পল মডেল দেখবে\n",
    "    validation_split=0.2, # ট্রেনিং ডেটার একটি অংশ ভ্যালিডেশনের জন্য ব্যবহার করা হবে\n",
    "    callbacks=[early_stopping], # EarlyStopping কলব্যাক যোগ করা হয়েছে\n",
    "    verbose=1\n",
    ")\n",
    "print(\"মডেল ট্রেনিং সম্পন্ন হয়েছে!\")\n",
    "\n",
    "# --- ধাপ ৭: মডেল মূল্যায়ন করো ---\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nটেস্টিং ডেটার উপর মডেলের নির্ভুলতা (Accuracy): {accuracy:.4f}\")\n",
    "print(f\"টেস্টিং ডেটার উপর মডেলের লস (Loss): {loss:.4f}\")\n",
    "\n",
    "# --- ধাপ ৮: নতুন রিভিউয়ের জন্য প্রেডিকশন করো ---\n",
    "new_reviews = [\n",
    "    \"This movie was truly amazing and captivating. Highly recommend!\",\n",
    "    \"Absolutely terrible, a complete waste of money and time. Avoid it.\",\n",
    "    \"It was an average film, nothing special.\",\n",
    "    \"The plot was confusing and the acting was wooden.\"\n",
    "]\n",
    "\n",
    "# নতুন রিভিউগুলোকে প্রসেস করো\n",
    "new_sequences = tokenizer.texts_to_sequences(new_reviews)\n",
    "new_padded = pad_sequences(new_sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# প্রেডিকশন করো\n",
    "predictions = model.predict(new_padded)\n",
    "\n",
    "print(\"\\nনতুন রিভিউয়ের প্রেডিকশন:\")\n",
    "for i, review in enumerate(new_reviews):\n",
    "    sentiment_score = predictions[i][0]\n",
    "    sentiment_label = \"Positive\" if sentiment_score >= 0.5 else \"Negative\"\n",
    "    print(f\"রিভিউ: '{review}'\")\n",
    "    print(f\"  অনুভূতির স্কোর: {sentiment_score:.4f} -> অনুমিত অনুভূতি: {sentiment_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
